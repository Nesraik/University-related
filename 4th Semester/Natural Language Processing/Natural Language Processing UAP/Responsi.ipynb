{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle # buat create model kita\n",
    "import string # buat ilangin tanda baca\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from random import shuffle # buat ngacak urutan dari dataset kita\n",
    "# nltk.download('punkt') # biar stemmer udah keload bahasanya\n",
    "# nltk.download('stopwords') # buat stopwordsnya\n",
    "import spacy  # Import spaCy for NER\n",
    "# Load spaCy's English model\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punc_list = set(string.punctuation)\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    word_list = word_tokenize(text.lower())\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    word_list = [word for word in word_list if word not in punc_list]\n",
    "    word_list = [word for word in word_list if word.isalpha()]\n",
    "    word_list = [stemmer.stem(word) for word in word_list]\n",
    "    word_list = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    \n",
    "    # freq_dist = FreqDist(word_list)\n",
    "    # words_features = [word for word, freq in freq_dist.most_common(100)]\n",
    "\n",
    "    return {word: True for word in word_list}\n",
    "\n",
    "def training_model(dataset):\n",
    "    features_set = [(preprocess(review), label) for title, review, label in zip(dataset['title'], dataset['review'], dataset['sentimentScore'])]\n",
    "    \n",
    "    shuffle(features_set)\n",
    "    split_index = int(len(features_set)*0.8)\n",
    "    train_set = features_set[:split_index]      \n",
    "    test_set = features_set[split_index:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    accuracy_train = accuracy(classifier, test_set)\n",
    "    print(f\"Accuracy: {accuracy_train*100}%\")\n",
    "    \n",
    "    file = open('model.pickle', 'wb')\n",
    "    pickle.dump(classifier, file)\n",
    "    file.close()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(dataset):\n",
    "    if os.path.exists('model.pickle'):\n",
    "        file = open('model.pickle', 'rb')\n",
    "        model = pickle.load(file)\n",
    "        file.close()\n",
    "    else:\n",
    "        model = training_model(dataset)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_recommendation(query, dataset):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    title_list = list(dataset['title'])\n",
    "    review_list = list(dataset['review'])\n",
    "    \n",
    "    matrix = vectorizer.fit_transform(review_list)\n",
    "    query_v = vectorizer.transform([query])\n",
    "    \n",
    "    similarities = cosine_similarity(query_v, matrix)\n",
    "    print(similarities.argsort()[-2:][::-1])\n",
    "    data = {'Title': title_list, 'Review': review_list, 'Similarity': similarities[0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    df_sorted = df.sort_values(by='Similarity', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    top_two_movies = []\n",
    "    \n",
    "    for index, row in df_sorted.iterrows():\n",
    "        title = row['Title']\n",
    "        \n",
    "        if title not in top_two_movies:\n",
    "            top_two_movies.append(title)\n",
    "        \n",
    "        if len(top_two_movies) == 2:\n",
    "            break\n",
    "    \n",
    "    for movie in top_two_movies:\n",
    "        print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_named_entities(dataset):\n",
    "    all_entities = {\n",
    "        \"LOC\": set(),\n",
    "        \"LANGUAGE\": set()\n",
    "    }\n",
    "    \n",
    "    for review in dataset['review']:\n",
    "        doc = nlp(review)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"LOC\":\n",
    "                all_entities[\"LOC\"].add(ent.text)\n",
    "            elif ent.label_ == \"LANGUAGE\":\n",
    "                all_entities[\"LANGUAGE\"].add(ent.text)\n",
    "    \n",
    "    \n",
    "    print(\"Summary of Named Entities:\")\n",
    "    for label, entities in all_entities.items():\n",
    "        entity = \", \".join(entities)\n",
    "        print(f\"{label}: {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOUR REVIEW :  this is my honest review, its actually good\n",
      "YOUR REVIEW CATEGORY :  POSITIVE\n",
      "\n",
      "Summary of Named Entities:\n",
      "LOC: Africa, Fantasy Island, Bay, Hippocratic, the Middle East, Rock Hudson, Red Planet, the North Pole, Isle of Dogs\n",
      "LANGUAGE: Hebrew\n",
      "[[784 750 410 409 408 751 405 753 754 401 399 398 397 391 413 758 387 760\n",
      "  385 384 383 381 764 766 377 376 373 769 371 388 369 414 749 463 462 461\n",
      "  460 458 457 909 453 735 736 448 447 737 416 738 739 740 438 437 742 430\n",
      "  429 428 743 746 747 420 418 442 733 367 774 797 800 804 299 298 297 296\n",
      "  295 290 807 808 809 813 307 283 815 279 816 274 273 272 819 267 266 263\n",
      "  262 259 820 281 361 795 310 355 354 777 778 351 350 348 347 346 341 339\n",
      "  338 782 309 783 333 330 659 328 785 325 323 320 319 790 315 313 793 334\n",
      "  257 729 468 608 677 678 605 680 603 601 600 681 598 597 596 595 609 594\n",
      "  590 589 588 686 687 581 578 577 691 573 571 568 567 683 699 675 616 656\n",
      "  655 662 653 650 648 646 645 644 643 641 664 665 615 638 636 635 634 633\n",
      "  632 666 630 629 628 627 623 672 674 637 466 701 702 505 503 502 500 499\n",
      "  498 718 496 493 492 491 720 489 506 723 485 484 482 481 480 478 726 475\n",
      "  474 472 471 470 727 487 561 507 716 559 558 557 556 553 552 550 549 547\n",
      "  546 705 542 538 717 536 531 530 528 713 524 523 521 520 518 517 715 515\n",
      "  514 535 821 658 134 142 859 139 860 137 822 133 862 131 128 127 143 126\n",
      "  863 865 866 121 870 871 118 116  30 114 113 125 873 144 857 183 182  26\n",
      "  180 179 177 174 173 850 171 854 145 167 855 161 160  27 155 154  48 151\n",
      "  149  28 147 164 845 111 109  73  71  70 888  68  67  66  65  64  40  61\n",
      "  886  41 893  58 892  57  44  56 890 891  46  47  52  59 110  76 884 108\n",
      "  107 874 105 102 101 100  32  96  93 879 895  91 880 881  86  34  85 882\n",
      "   83 883  80  36  79  90 189 153  23 252 240 905  13 218 239 244 831 903\n",
      "  197 198 840 898   8 203 199 246 834 900  14 226 222 223   7  15 225 248\n",
      "   18 828 897  24 242 215 835 236 217 241 839 208 844 238 190 192 232   4\n",
      "  906 827 602 649 724 451 651 709 555 837 446 117  33 265 748 565 755   5\n",
      "  152 331 356 889 483  75 684 676 584 318 856 543 389  82  63 504 363 652\n",
      "  301  99 671 286 745 833 150 761 178  53 619  38 235 818 625  98  78 569\n",
      "    3 157 261 249 757 823 213 165 206 175 529 130 708 321  39 706  94  22\n",
      "   60 245 512 303 878 186 690 849 264 896 532 324 277 775 294 508 806 433\n",
      "  479 541 776 202 805 166 103 799 221 392 360 393  51   9 345 741  55 696\n",
      "  336 184 861 140 370 316 848 902 251  19 129 364 838 467 159 260 412 268\n",
      "  752 551 243 768 135 394 459 195 533 432 423 400  49 812 611 176  35 276\n",
      "  456 436 425 449 494 789 703 756 669 511 211 670 614 786  31 908 427 509\n",
      "  332 362 148 842 734 204 624 344 233 647 444 714 431 667 910 685 772 162\n",
      "  663 368 501 196 544  72 692 580  50 767 312 201 473 780 158 115 688 256\n",
      "  877 852  87 830 193 247 792 200 566 697 779 707 814 228 214 270 185 773\n",
      "  207  97  10 824 349  16 562 586 304 522 704 825 698 434  17 382 613 579\n",
      "  853 495 817 219 712 231 486 415 781 534 292   0 366 732 469 661 682 411\n",
      "  901  74 188 585 537 258  25 796 620 899 352 314 327 375 695 570 169 230\n",
      "  798 326  62 639 288 811 168 759 607 832 673 342 846 710 163 293 869 209\n",
      "  395 227 513 622 526 864 867 907 291 554 762 343  11 337  45 357  43 122\n",
      "  591 205 563 803 287 851 575 722 275 548 454 146 421 574 278 490 172 576\n",
      "  440 519 847 141   2 253 305 250 212 455 829 525 106 285 439 284 300 358\n",
      "  426 872 269 124 194 610 728 700 826 725 329 841 836 887 564   1 560 443\n",
      "  220  81 378  88 224 618 679 582  89 120 660 365 731 763 612 668 885 136\n",
      "  317 379 380  69 403 386 390 187  84 626 744   6 210  37 289 719 112 801\n",
      "  604 452  21 843 280 787 237 340 711 138 156  92 311 858 374 868 359 464\n",
      "  302 693 119 721 527 894 476 657 592 694 621 794  54 465 730 404  77 170\n",
      "  497 216 572 488 181 587 876 271 599  42 765 308 306 396 450 606 132 445\n",
      "   95 104 254 545 583 353 422 322 642 689 372 640 123 770 417 516 419 802\n",
      "  335 875 441 435 407 617 234  29 593 255 191 771 424 477 654 229 402 510\n",
      "  540  12  20 539 788 810 631 791 406 904 282]]\n",
      "The Life Aquatic With Steve Zissou\n",
      "Enduring Love\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Movie Dataset.csv').sample(n=1000)\n",
    "dataset.dropna(inplace=True)\n",
    "classifier = load_model(dataset)\n",
    "\n",
    "review = \"this is my honest review, its actually good\"\n",
    "review_tokenized = word_tokenize(review)\n",
    "category = classifier.classify(FreqDist(review_tokenized))\n",
    "print(\"YOUR REVIEW : \", review)\n",
    "print(\"YOUR REVIEW CATEGORY : \", category, end=\"\\n\\n\")\n",
    "\n",
    "categorize_named_entities(dataset)\n",
    "movie_recommendation(review, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
