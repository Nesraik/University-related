{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingDataset():\n",
    "    data = pd.read_csv('Movie Dataset.csv').sample(n=1000)\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "eng_stop_words = stopwords.words('english')\n",
    "symbol = punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(tag):\n",
    "    if tag == 'jj':\n",
    "        return 'a'\n",
    "    elif tag in ['nn','rb','vb']:\n",
    "        return tag[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatizing(word_list):\n",
    "    tagging  = pos_tag(word_list)\n",
    "    lemmatized_list = []\n",
    "    for word, tag in tagging:\n",
    "        label = get_label(tag)\n",
    "        if label != None:\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word,label))\n",
    "        else:\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessed_text(data): \n",
    "    reviews = data['review'].to_list()\n",
    "    sentiments = data['sentimentScore'].to_list()\n",
    "\n",
    "    word_list = []\n",
    "\n",
    "    for sentence in reviews:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            word_list.append(word.lower())\n",
    "\n",
    "    word_list = [word.lower() for word in word_list if word not in symbol and word not in eng_stop_words and word.isalpha()]\n",
    "    word_list = lemmatizing(word_list)\n",
    "\n",
    "    feature_set = []\n",
    "\n",
    "    labeled_list = list(zip(reviews,sentiments))\n",
    "\n",
    "    for sentence, label in labeled_list:\n",
    "        words = []\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [word.lower() for word in words if word not in symbol and word not in eng_stop_words and word.isalpha()]\n",
    "        words = lemmatizing(words)\n",
    "\n",
    "        feature = {}\n",
    "\n",
    "        for word in words:\n",
    "            feature[word] = (word in word_list)\n",
    "        feature_set.append((feature,label))\n",
    "\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling():\n",
    "    data = loadingDataset()\n",
    "    feature_set = Preprocessed_text(data)\n",
    "    shuffle(feature_set)\n",
    "    training_count = int(0.8*len(feature_set))\n",
    "    training_data = feature_set[:training_count]\n",
    "    testing_data = feature_set[training_count:]\n",
    "    \n",
    "    model = NaiveBayesClassifier.train(training_data)\n",
    "\n",
    "    file = open('model.pickle','wb')\n",
    "    pickle.dump(model,file)\n",
    "    file.close()\n",
    "\n",
    "    print(model.show_most_informative_features(n=10))\n",
    "    print(accuracy(model,testing_data))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkModel():\n",
    "    try:\n",
    "        file = open('model.pickle','rb')\n",
    "        model = pickle.load(file)\n",
    "        file.close()\n",
    "    except:\n",
    "        model = modelling()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeReview(model):\n",
    "    while True:\n",
    "        review = input('Write your review: ')\n",
    "        reviews = word_tokenize(review)\n",
    "        if (len(reviews)>=20):\n",
    "            break\n",
    "    category = model.classify(FreqDist(reviews))\n",
    "    return review,category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieRecommendation(data,user_review):\n",
    "    review = list(data['review'])\n",
    "    vectorizer  = TfidfVectorizer()\n",
    "    data_vec = vectorizer.fit_transform(review)\n",
    "    user_vec = vectorizer.transform([user_review])\n",
    "\n",
    "    similarities = cosine_similarity(user_vec,data_vec).flatten()\n",
    "    top_indices = similarities.argsort()[-2:][::-1]\n",
    "\n",
    "    counter = 1\n",
    "    for index in top_indices:\n",
    "        print(f\"{counter}: {data.iloc[index]['title']}\")\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER(data):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    category = {\n",
    "        'LOC' : set(),\n",
    "        'LANGUAGE': set()\n",
    "    }\n",
    "\n",
    "    for review in data['review']:\n",
    "        doc = nlp(review)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'LOC':\n",
    "                category['LOC'].add(ent.text)\n",
    "            elif ent.label_ == 'LANGUAGE':\n",
    "                category['LANGUAGE'].add(ent.text)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    for label, text in category.items():\n",
    "        print(f\"{label}: {', '.join(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOVIE RECOMMENDATION APPLICATION BASED ON REVIEWS\n",
      "YOUR REVIEW: NO REVIEW\n",
      "YOUR REVIEW CATEGORY: UNKNOWN\n",
      "1. WRITE YOUR REVIEW\n",
      "2. MOVIE RECOMMENDATION\n",
      "3. NER\n",
      "4. EXIT\n",
      "LOC: The Big Short, Berry and Stone, Northern California, Wind River, Asia, Hudson, Paradise, Soho, The Wild Bunch&#44\n",
      "LANGUAGE: English\n",
      "MOVIE RECOMMENDATION APPLICATION BASED ON REVIEWS\n",
      "YOUR REVIEW: NO REVIEW\n",
      "YOUR REVIEW CATEGORY: UNKNOWN\n",
      "1. WRITE YOUR REVIEW\n",
      "2. MOVIE RECOMMENDATION\n",
      "3. NER\n",
      "4. EXIT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3. NER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4. EXIT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m option \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m>> \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     18\u001b[0m     user_review, category \u001b[38;5;241m=\u001b[39m writeReview(model)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "user_review = 0\n",
    "category = 0\n",
    "\n",
    "model = checkModel()\n",
    "data = loadingDataset()\n",
    "\n",
    "while True:\n",
    "    print('MOVIE RECOMMENDATION APPLICATION BASED ON REVIEWS')\n",
    "    print(f\"YOUR REVIEW: {'NO REVIEW' if user_review == 0 else user_review}\")\n",
    "    print(f\"YOUR REVIEW CATEGORY: {'UNKNOWN' if category == 0 else category}\")\n",
    "    print('1. WRITE YOUR REVIEW')\n",
    "    print('2. MOVIE RECOMMENDATION')\n",
    "    print('3. NER')\n",
    "    print('4. EXIT')\n",
    "    option = int(input('>> '))\n",
    "\n",
    "    if option == 1:\n",
    "        user_review, category = writeReview(model)\n",
    "    elif option == 2:\n",
    "        movieRecommendation(data,user_review)\n",
    "    elif option == 3:\n",
    "        NER(data)\n",
    "    elif option == 4:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Natural_Language_Processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
