{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset():\n",
    "    Dataset = pd.read_csv(\"IMDB Dataset.csv\").sample(n=100)\n",
    "    review_list = Dataset['review'].to_list()\n",
    "    sentiment_list = Dataset['sentiment'].to_list()\n",
    "    return review_list,sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making List of element that is going to be removed from data\n",
    "eng_stop_words = stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "#Tools for preprocessing\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing\n",
    "\n",
    "def removestopwords(word_list):\n",
    "    return [word.lower() for word in word_list if word.lower() not in eng_stop_words]\n",
    "\n",
    "def removesymbol(word_list):\n",
    "    return [word for word in word_list if word not in punctuation]\n",
    "\n",
    "def removenumber(word_list):\n",
    "    return [word for word in word_list if not word.isnumeric()]\n",
    "\n",
    "def stemmingword(word_list):\n",
    "    return [stemmer.stem(word) for word in word_list]\n",
    "\n",
    "def get_tag(label):\n",
    "    if label == 'jj':\n",
    "        return 'a'\n",
    "    elif label in ['nn','rb','vb']:\n",
    "        return label[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatizingword(word_list):\n",
    "    lemmatized_list = []\n",
    "    tagging = pos_tag(word_list)\n",
    "    for word, tag in tagging:\n",
    "        label =  get_tag(tag)\n",
    "        if label != None:\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word,label))\n",
    "        else:\n",
    "            lemmatized_list.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "def dataPreprocessing(review_list,sentiment_list):\n",
    "    word_list = []\n",
    "\n",
    "    for sentence in review_list:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            word_list.append(word.lower())\n",
    "\n",
    "    word_list = removestopwords(word_list)\n",
    "    word_list = removesymbol(word_list)\n",
    "    word_list = removenumber(word_list)\n",
    "    #word_list = stemmingword(word_list)\n",
    "    word_list = lemmatizingword(word_list)\n",
    "\n",
    "    labeled_list = list(zip(review_list,sentiment_list))\n",
    "\n",
    "    feature_set = []\n",
    "\n",
    "    for sentence, label in labeled_list:\n",
    "        feature = {}\n",
    "        words_list = []\n",
    "        words_list = word_tokenize(sentence)\n",
    "        words_list = removestopwords(words_list)\n",
    "        words_list = removesymbol(words_list)\n",
    "        words_list = removenumber(words_list)\n",
    "        #words_list = stemmingword(words_list)\n",
    "        words_list = lemmatizingword(words_list)\n",
    "\n",
    "        for word in words_list:\n",
    "            feature[word] = (word in word_list)\n",
    "        feature_set.append((feature,label))\n",
    "    return feature_set    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using model to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildingModel():\n",
    "    review_list, sentiment_list = loadDataset()\n",
    "    feature_set = dataPreprocessing(review_list,sentiment_list)\n",
    "    random.shuffle(feature_set)\n",
    "    training_count = int(len(feature_set)*0.8)\n",
    "    train_data = feature_set[:training_count]\n",
    "    test_data = feature_set[training_count:]\n",
    "\n",
    "    classifer = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    file = open(\"model.pickle\",\"wb\")\n",
    "    pickle.dump(classifer,file)\n",
    "    file.close()\n",
    "\n",
    "    print(classifer.show_most_informative_features(n=5))\n",
    "    print(\"Training Accuracy:\",accuracy(classifer,test_data))\n",
    "    \n",
    "    print(\"Training Model Complete...\")\n",
    "    input(\"Press enter to continue...\")\n",
    "    return classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find existing model in file's directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findModel():\n",
    "    try:\n",
    "        file = open(\"model.pickle\",\"rb\")\n",
    "        classifier = pickle.load(file)\n",
    "        file.close()\n",
    "    except:\n",
    "        classifier = buildingModel()\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTweet():\n",
    "    tweet = ''\n",
    "    while True:\n",
    "        tweet = input(\"Input the tweet (must contain atleast 5 words): \")\n",
    "        if(len(tweet.split(\" \"))>=5):\n",
    "            break\n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeTweet(tweet,classifer):\n",
    "    if tweet is None:\n",
    "        print(\"Tweet doesnot exist\")\n",
    "        return\n",
    "    words = word_tokenize(tweet)\n",
    "    tagging = pos_tag(words)\n",
    "    print(\"Tweet Part Of Speech Tag :\")\n",
    "    indexnum = 1\n",
    "    for word, tag in tagging:\n",
    "        print(f\"{indexnum}. {word} : {tag}\")\n",
    "        indexnum+=1\n",
    "    input(\"Press enter to continue...\")\n",
    "    for word in words:\n",
    "        print(f\"Word: {word}\")\n",
    "        print(\"---------------------------------------\")\n",
    "        synset = wordnet.synsets(word)\n",
    "        try:\n",
    "            syn = synset[0]\n",
    "            synonym = syn.lemmas()[0]\n",
    "            antonym = synonym.antonyms()[0]\n",
    "            print(\"Synonym\")\n",
    "            print(f\"(+){synonym.name()}\")\n",
    "            print(\"         Antonym\")\n",
    "            print(f\"        (-){antonym.name()}\")\n",
    "            print(\"---------------------------------------\")\n",
    "        except:\n",
    "            print(\"The word doesnot have any synonym or antonym\")\n",
    "            print(\"-------------------------------------------------\")\n",
    "    print(\"Tweet Category :\",classifer.classify(FreqDist(words)))\n",
    "    input(\"Press enter to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "Tweet Part Of Speech Tag :\n",
      "1. Monkey : NNP\n",
      "2. can : MD\n",
      "3. not : RB\n",
      "4. improve : VB\n",
      "5. himself : PRP\n",
      "6. stupid : JJ\n",
      "Word: Monkey\n",
      "---------------------------------------\n",
      "The word doesnot have any synonym or antonym\n",
      "-------------------------------------------------\n",
      "Word: can\n",
      "---------------------------------------\n",
      "The word doesnot have any synonym or antonym\n",
      "-------------------------------------------------\n",
      "Word: not\n",
      "---------------------------------------\n",
      "The word doesnot have any synonym or antonym\n",
      "-------------------------------------------------\n",
      "Word: improve\n",
      "---------------------------------------\n",
      "Synonym\n",
      "(+)better\n",
      "         Antonym\n",
      "        (-)worsen\n",
      "---------------------------------------\n",
      "Word: himself\n",
      "---------------------------------------\n",
      "The word doesnot have any synonym or antonym\n",
      "-------------------------------------------------\n",
      "Word: stupid\n",
      "---------------------------------------\n",
      "The word doesnot have any synonym or antonym\n",
      "-------------------------------------------------\n",
      "Tweet Category : negative\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n",
      "1. Write tweet\n",
      "2. Analyze tweet\n",
      "3. Exit\n"
     ]
    }
   ],
   "source": [
    "classifer = findModel()\n",
    "tweet = ''\n",
    "\n",
    "while True:\n",
    "    print(\"1. Write tweet\")\n",
    "    print(\"2. Analyze tweet\")\n",
    "    print(\"3. Exit\")\n",
    "    choice = input()\n",
    "\n",
    "    match choice:\n",
    "        case \"1\":\n",
    "            tweet = writeTweet()\n",
    "        case \"2\":\n",
    "            analyzeTweet(tweet,classifer)\n",
    "        case \"3\":\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Natural_Language_Processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
